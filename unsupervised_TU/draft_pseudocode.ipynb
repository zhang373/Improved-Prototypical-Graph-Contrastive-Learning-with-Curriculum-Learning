{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伪代码\n",
    "\n",
    "```python\n",
    "for G in dataloader:\n",
    "```\n",
    "### &emsp;数据增强\n",
    "```python\n",
    "    G1, G2 = T1(G), T2(G)\n",
    "```\n",
    "\n",
    "### &emsp;表示计算\n",
    "```python\n",
    "    z = GNN(cat(G1, G2))\n",
    "```\n",
    "\n",
    "### &emsp;prototypes打分计算\n",
    "```python\n",
    "    prot_scores = torch.mm(z, model.prototypes)\n",
    "    prot_scores1, prot_scores2 = prot_scores[:N], prot_scores[-N:]\n",
    "```\n",
    "\n",
    "### &emsp;根据Sinkhorn计算Q1, Q2\n",
    "```python\n",
    "    with torch.no_grad():\n",
    "        Q1 = Sinkhorn(prot_scores1)\n",
    "        Q2 = Sinkhorn(prot_scores2)\n",
    "```\n",
    "\n",
    "### &emsp;根据打分归一化得到预测概率P1, P2；为各图选择概率最大的prototypes作为原型assignment，即得到prot_assign\n",
    "```python\n",
    "    P1 = softmax(prot_scores1/ temp)\n",
    "    # prot_assign1 = torch.argmax(P1, dim=1)\n",
    "    P2 = softmax(prot_scores2/ temp)\n",
    "    # prot_assign2 = torch.argmax(P2, dim=1)\n",
    "```\n",
    "\n",
    "### &emsp;采用DBSCAN对model.prototypes进行聚类，得到各样本的聚类assignment，即得到cluster_assign\n",
    "```python\n",
    "    # ！注意！DBSCAN返回的label array中可能包含-1，-1意味着对应位置的sample是noisy sample（即：根据目前选定的eps和min_samples无法将其与其他samples聚类）\n",
    "    # 此处默认prot_2_cluster_dict中的cluster idx取值范围是0~(cluster_num-1)之间的连续整数\n",
    "    prot_2_cluster_dict, cluster_num = DBSCAN(eps=epsilon, min_samples=min_samples).fit(model.prototypes) \n",
    "    \n",
    "    # cluster_P1 = prob_agg(prot_2_cluster_dict, P1, cluster_num)\n",
    "    # cluster_P2 = prob_agg(prot_2_cluster_dict, P2, cluster_num)\n",
    "    cluster_score1 = score_agg(prot_2_cluster_dict, prot_scores1, cluster_num)\n",
    "    cluster_score2 = score_agg(prot_2_cluster_dict, prot_scores2, cluster_num)\n",
    "    cluster_score = cat([cluster_score1, cluster_score2])\n",
    "    cluster_P1 = softmax(cluster_score1/ temp)\n",
    "    cluster_P2 = softmax(cluster_score2/ temp)\n",
    "    cluster_P = cat([cluster_P1, cluster_P2])\n",
    "    cluster_assign1 = torch.argmax(cluster_P1, dim=1)\n",
    "    cluster_assign2 = torch.argmax(cluster_P2, dim=1)\n",
    "    # cluster_assign1 = get_cluster_assign(prot_2_cluster_dict, prot_assign1)\n",
    "    # cluster_assign2 = get_cluster_assign(prot_2_cluster_dict, prot_assign2)\n",
    "    cluster_assign = cat([cluster_assign1, cluster_assign2])\n",
    "\n",
    "    # 根据Sinkhorn计算cluster_Q1, cluster_Q2\n",
    "    with torch.no_grad():\n",
    "        cluster_Q1 = Sinkhorn(cluster_score1)\n",
    "        cluster_Q2 = Sinkhorn(cluster_score2)\n",
    "```\n",
    "\n",
    "### &emsp;计算clustering consistency loss，基于（P,Q）或者（cluster_P， cluster_Q）\n",
    "```python\n",
    "    L_cluster_consistency = -0.5(cluster_Q1 log cluster_P2 + cluster_Q2 log cluster_P1) # L_cluster_consistency = -0.5(Q1 logP2 + Q2 logP1)\n",
    "```\n",
    "\n",
    "### &emsp;根据prototypes/聚类概率，计算各样本的信息熵并判断各样本是否可信\n",
    "```python\n",
    "    info_ent1 = get_info_ent(cluster_P1) # info_ent1 = get_info_ent(P1)\n",
    "    info_ent2 = get_info_ent(cluster_P2) # info_ent2 = get_info_ent(P2)\n",
    "    info_ent = cat([info_ent1, info_ent2])\n",
    "    info_ent_avg = mean(info_ent)\n",
    "\n",
    "    reliab_num = reliab_pacing(info_ent_avg, args.info_ent_threshold, args.reliab_pacing_type, sample_num, t, T) # pacing function\n",
    "    reliab_mask, reliab_idx = get_relib_mask(info_ent1, info_ent2, reliab_num)\n",
    "```\n",
    "\n",
    "### &emsp;根据样本是否可信、聚类结果以及是否来自同个锚图，选定【待选正样本对】pos_cand_mask; 进一步根据正样本对的【双向KL散度】或【JS散度】确定要选取的正样本对reliab_pos_mask\n",
    "```python\n",
    "    graph_num = sample_num/2\n",
    "    # 来自同一个锚图的两个视图均可信时，二者构成的正样本对才是可信的\n",
    "    pos_cand_mask = reliab_mask[:graph_num] & reliab_mask[-graph_num:]\n",
    "\n",
    "    # 来自同一个锚图的两个视图能聚到同一类时，二者构成的正样本对才是可信的\n",
    "    assgin_mask = [cluster_assign1[i] == cluster_assign2[i] for i in range(graph_num)] # assgin_mask = [prot_assign1[i] == prot_assign2[i] for i in range(graph_num)]\n",
    "    pos_cand_mask = pos_cand_mask & assgin_mask\n",
    "    pos_cand_num = pos_cand_mask.count_nonzero()\n",
    "\n",
    "    # 计算待选正样本对聚类概率的差异度，即：【双向KL散度】或【JS散度】；非待选负样本对的位置置为inf\n",
    "    pos_div = [get_div(cluster_P1[i], cluster_P2[i]) if pos_cand_mask[i]==True else inf for i in range(graph_num)] \n",
    "    pos_cand_div_avg = sum(pos_div * pos_cand_mask) / pos_cand_num\n",
    "\n",
    "    reliab_pos_num = reliab_pacing(pos_cand_div_avg, args.pos_div_threshold, args.pos_reliab_pacing_type, pos_cand_num, t, T)\n",
    "    reliab_pos_mask, reliab_pos_idx = get_reliab_pos_mask(pos_div, reliab_pos_num)\n",
    "```\n",
    "\n",
    "### &emsp;根据样本是否可信、是否被选作正样本、聚类结果是否不同以及是否来自不同的锚图，选定【待选负样本对】neg_cand_mask; 进一步根据负样本对的【双向KL散度】或【JS散度】确定要选取的负样本对reliab_neg_mask\n",
    "```python\n",
    "    # 初始化neg_cand_mask\n",
    "    neg_cand_mask = torch.zeros(sample_num, sample_num).bool()\n",
    "    # 计算待选负样本对聚类概率的差异度，即：【双向KL散度】或【JS散度】；非待选负样本对的位置置为0\n",
    "    neg_div = torch.zeros(sample_num, sample_num)\n",
    "    # 当样本i拥有正样本对时，才为其选择负样本\n",
    "    for i in reliab_pos_idx:\n",
    "        # 当样本j是可信样本时，才有资格被选为负样本\n",
    "        for j in reliab_idx:\n",
    "            not_self = (i != j)\n",
    "            not_same_graph = (i != j+graph_num)\n",
    "            i_cluster_idx, j_cluster_idx = cluster_assign[i], cluster_assign[j] #样本i,j的所属类别的索引\n",
    "            not_same_cluster = (i_cluster_idx != j_cluster_idx) # not_same_cluster = (prot_assign[i] != prot_assign[j])\n",
    "            if not_self and not_same_graph and not_same_cluster:\n",
    "                neg_cand_mask[i][j] = True\n",
    "                # 只关心负样本对二者所属类别上的概率差异度\n",
    "                ij_cluster_idx  = [i_cluster_idx, j_cluster_idx]\n",
    "                score_i = cluster_score[i][ij_cluster_idx]\n",
    "                score_j = cluster_score[j][ij_cluster_idx]\n",
    "                p_i = softmax(score_i/ temp)\n",
    "                p_j = softmax(score_j/ temp)\n",
    "                neg_div[i][j] = get_div(p_i, p_j)\n",
    "\n",
    "    neg_cand_num = neg_cand_mask.count_nonzero()\n",
    "    neg_cand_div_avg = sum(neg_div) / neg_cand_num\n",
    "\n",
    "    # ****************已与上述代码块进行合并****************\n",
    "    # # 计算待选负样本对聚类概率的差异度，即：【双向KL散度】或【JS散度】；非待选负样本对的位置置为0\n",
    "    # neg_div = torch.zeros(sample_num, sample_num)\n",
    "    # for i in reliab_pos_idx:\n",
    "    #     for j in reliab_idx:\n",
    "    #         if neg_cand_mask[i][j] == True:\n",
    "    #             ij_cluster_idx = [cluster_assign[i], cluster_assign[j]]\n",
    "    #             neg_div[i][j] = get_div(cluster_P[i][ij_cluster_idx], cluster_P[j][ij_cluster_idx])\n",
    "    # neg_cand_div_avg = sum(neg_div) / neg_cand_num\n",
    "\n",
    "    reliab_neg_num = reliab_neg_pacing(neg_cand_div_avg, args.neg_div_threshold, args.neg_reliab_pacing_type,neg_cand_num, t, T)\n",
    "    reliab_neg_mask, reliab_neg_row_idx, reliab_neg_col_idx = get_reliab_neg_mask(neg_div, reliab_neg_num)\n",
    "    # \n",
    "```\n",
    "\n",
    "### &emsp;计算负样本权重：使得距离适中的负样本权重最大\n",
    "```python\n",
    "    # ##--1） 基于所有样本对的距离来评估负样本对的距离是否适中计算reweight\n",
    "    # # 即：平均值、标准差针对所有样本对进行计算，而不是计算所有可行负样本对的平均值、标准差，在此基础上再计算reweight\n",
    "    # # 两两样本差异度矩阵计算\n",
    "    # sample_dist = 1 - torch.mm(z, z.t().contiguous())\n",
    "    # mu=torch.mean(sample_dist,dim=1)\n",
    "    # std = torch.std(sample_dist,dim=1)\n",
    "    # reweight = torch.exp(-torch.pow(sample_dist - mu,2)/(2 * torch.pow(std,2))).to(device)\n",
    "    # reweight= reweight * reliab_neg_mask\n",
    "\n",
    "    ##--2) 基于可信负样本对来衡量各负样本对的距离是否适中计算reweight，即:计算所有可行负样本对的平均值、标准差，在此基础上再计算reweight\n",
    "    # 两两样本差异度矩阵计算\n",
    "    sample_dist = 1 - torch.mm(z, z.t().contiguous())\n",
    "    mu = torch.sum(sample_dist*reliab_neg_mask, dim=1) / torch.sum(reliab_neg_mask, dim=1) # 注意除0错误\n",
    "    reshape_mu = torch.reshape(mu, (-1,))\n",
    "    temp_res = torch.pow(sample_dist.sub(reshape_mu[:, None]) , 2) * reliab_neg_mask # ((sample_dist-mu)**2)* reliab_neg_mask: sample_dist第i行的每个元素都减去mu[i]\n",
    "    std = torch.sqrt( temp_res / torch.sum(reliab_neg_mask, dim=1) ) # 注意除0错误\n",
    "    reweight = torch.exp(-torch.pow(sample_dist - mu,2)/(2 * torch.pow(std,2))).to(device)\n",
    "    reweight= reweight * reliab_neg_mask\n",
    "\n",
    "    # 计算权重矩阵每行的归一化系数\n",
    "    reweight_normalize = torch.sum(reliab_neg_mask) / torch.sum(reweight, dim=1) # 逐元素的除法,reweight_normalize的size应该是(reweight.size()[0], 1)\n",
    "    reweight = reweight * reweight_normalize.reshape((reweight.size()[0], 1))\n",
    "\n",
    "\n",
    "    # 计算样本对的相似度并且进行加权\n",
    "    sim_matrix  = torch.exp(torch.mm(z, z.t().contiguous()) / temperature)\n",
    "    sim_matrix = (sim_matrix * reweight) * reliab_neg_mask\n",
    "    # 计算正样本对相似度\n",
    "    pos_sim = torch.exp(torch.sum(z[:sample_num] * z[-sample_num:],dim=-1) / temperature)\n",
    "    pos_sim = pos_sim * reliab_pos_mask\n",
    "    pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "\n",
    "    # 计算对比损失L_contrastive\n",
    "    L_contrastive = -(torch.log((pos_sim / (pos_sim + sim_matrix.sum(dim=-1)))[reliab_pos_idx])).mean()\n",
    "\n",
    "```\n",
    "\n",
    "### &emsp;计算聚类正则化项L_cluster_reg\n",
    "```python\n",
    "    cluster_2_protList_dict = {cluster:[] for cluster in range(cluster_num)} #value初始化为[]\n",
    "    for prot, cluster in prot_2_cluster_dict:\n",
    "        cluster_2_protList_dict[cluster].append(prot)\n",
    "\n",
    "    # 得到prot_pos_mask和prot_neg_mask，尺寸都是prototype_num x prototype_num，prototype_num：prototypes总数目\n",
    "    prot_pos_mask = torch.zeros(prototype_num, prototype_num)\n",
    "    # prot_neg_mask = torch.ones(prototype_num, prototype_num)\n",
    "    for cluster in range(cluster_num):\n",
    "        prot_list = cluster_2_protList_dict[cluster]\n",
    "        prot_pos_mask[prot_list][:, prot_list] = 1 #将属于同一个cluster的prototypes作为正样本（此处自身也被选为自己的正样本）\n",
    "        # prot_neg_mask[prot_list][:, prot_list] = 0\n",
    "    prot_neg_mask = torch.ones(prototype_num, prototype_num) - prot_pos_mask #将不属于同一个cluster的prototypes都选为负样本\n",
    "\n",
    "    prot_pos_mask = prot_pos_mask.bool()\n",
    "    prot_pos_mask[range(prototype_num), range(prototype_num)] = False #不能将自身作为自己的正样本\n",
    "    prot_neg_mask = prot_neg_mask.bool()\n",
    "\n",
    "    ## 此处也可以类似前面的--1)去计算加权矩阵\n",
    "    ##--2) 基于可信负样本对来衡量各负样本对的距离是否适中计算reweight，即:计算所有可行负样本对的平均值、标准差，在此基础上再计算reweight\n",
    "    # 两两样本差异度矩阵计算\n",
    "    prot_dist = 1 - torch.mm(model.prototypes, model.prototypes.t().contiguous())\n",
    "    mu_ = torch.sum(prot_dist*prot_neg_mask, dim=1) / torch.sum(prot_neg_mask, dim=1) # 注意除0错误\n",
    "    reshape_mu_ = torch.reshape(mu_, (-1,))\n",
    "    temp_res_ = torch.pow(prot_dist.sub(reshape_mu_[:, None]) , 2) * prot_neg_mask # ((prot_dist-mu_)**2)* prot_neg_mask: prot_dist第i行的每个元素都减去mu[i]\n",
    "    std_ = torch.sqrt( temp_res_ / torch.sum(prot_neg_mask, dim=1) ) # 注意除0错误\n",
    "    prot_reweight = torch.exp(-torch.pow(prot_dist - mu_,2)/(2 * torch.pow(std_,2))).to(device)\n",
    "    prot_reweight= prot_reweight * prot_neg_mask\n",
    "\n",
    "    # 计算权重矩阵每行的归一化系数\n",
    "    prot_reweight_normalize = torch.sum(prot_neg_mask) / torch.sum(prot_reweight, dim=1) # 逐元素的除法,prot_reweight_normalize的size应该是(prot_reweight.size()[0], 1)\n",
    "    prot_reweight = prot_reweight * prot_reweight_normalize.reshape((prot_reweight.size()[0], 1))\n",
    "\n",
    "    # 计算prot_sim_matrix，加权并根据prot_neg_mask得到得到prot_neg_sim_matrix，根据prot_pos_mask得到prot_pos_sim_matrix\n",
    "    prot_sim_matrix = torch.exp(torch.mm(model.prototypes, model.prototypes.t().contiguous()) / temperature)\n",
    "    prot_neg_sim_matrix = (prot_sim_matrix * prot_reweight) * prot_neg_mask\n",
    "    prot_pos_sim_matrix = prot_sim_matrix * prot_pos_mask\n",
    "\n",
    "    # 计算L_cluster_reg，由于分子中有多个正样本对，所以要进行归一化处理，即：分子乘 1/prot_pos_mask.sum(dim=-1)\n",
    "    L_cluster_reg = -( torch.log(( (prot_pos_sim_matrix.sum(dim=-1) / prot_pos_mask.sum(dim=-1)) / (prot_pos_sim_matrix.sum(dim=-1) + prot_neg_sim_matrix.sum(dim=-1)) )) ).mean()\n",
    "    \n",
    "```\n",
    "\n",
    "### &emsp;计算最终的损失函数L\n",
    "```python\n",
    "L = L_contrastive + args.lambda_1 * L_cluster_consistency + args.lambda_2 * L_cluster_reg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数定义\n",
    "```python\n",
    "# def get_cluster_assign(prot_2_cluster_dict, prot_assign):\n",
    "#     cluster_assign = [prot_2_cluster_dict[i] for i in prot_assign]\n",
    "#     return cluster_assign\n",
    "\n",
    "# def prob_agg(prot_2_cluster_dict, prot_prob, cluster_num):\n",
    "#     cluster_prob = [0 for i in cluster_num]\n",
    "#     for prot in range(len(prot_prob)):\n",
    "#         cluster = prot_2_cluster_dict[prot]\n",
    "#         cluster_prob[cluster] = max(cluster_prob[cluster], prot_prob[prot])\n",
    "#     return cluster_prob\n",
    "\n",
    "def score_agg(prot_2_cluster_dict, prot_score, cluster_num):\n",
    "    cluster_score = [0 for i in cluster_num]\n",
    "    for prot in range(len(prot_score)):\n",
    "        cluster = prot_2_cluster_dict[prot]\n",
    "        cluster_score[cluster] = max(cluster_score[cluster], prot_score[prot])\n",
    "    return cluster_score\n",
    "\n",
    "def reliab_pacing(avg, threshold, pacing_type, sample_num, t, T):\n",
    "    # 模型的相对容量\n",
    "    relative_cap = max(1, threshold/avg) # relative_cap = threshold / (threshold + avg)\n",
    "\n",
    "    # 根据pacing type进行计算，pacing type \\in [logarithmic, polynomial_1, polynomial_2, polynomial_3]\n",
    "    if pacing_type==\"logarithmic\":\n",
    "        reliab_num = (1 + 0.1 log(relative_cap * (t/T) + e ** (-10))) * sample_num\n",
    "    elif pacing_type==\"polynomial_1\":\n",
    "        reliab_num = (relative_cap * (t/T)) * sample_num\n",
    "    elif pacing_type==\"polynomial_2\":\n",
    "        reliab_num = (relative_cap * (t/T))**2 * sample_num\n",
    "    elif pacing_type==\"polynomial_3\":\n",
    "        reliab_num = (relative_cap * (t/T))**3 * sample_num\n",
    "    else:\n",
    "        error\n",
    "    return reliab_num\n",
    "\n",
    "def get_relib_mask(info_ent1, info_ent2, reliab_num):\n",
    "    info_ent = cat([info_ent1, info_ent2])\n",
    "    reliab_idx = info_ent.argsort()[-reliab_num:] # 选取信息熵最低的realib_num个样本作为可信样本\n",
    "\n",
    "    sample_num = len(info_ent)\n",
    "    reliab_mask = [False for i in range(sample_num)] # 初始化 \n",
    "    reliab_mask[reliab_idx] = True # 将可信样本处置为True\n",
    "\n",
    "    # # 当且仅当来自同一个锚图的两个视图样本均为可信时二者才是可信的，若只有一个可信则无法组成可信的正样本对\n",
    "    # reliab_mask1, reliab_mask2 = reliab_mask[:sample_num/2], reliab_mask[-sample_num/2:]\n",
    "    # reliab_mask = reliab_mask1 & reliab_mask2 \n",
    "    # reliab_mask = cat([reliab_mask, reliab_mask])\n",
    "\n",
    "    return reliab_mask, reliab_idx\n",
    "\n",
    "def get_reliab_pos_mask(pos_div, reliab_pos_num): # 类似get_relib_mask()\n",
    "    reliab_pos_idx = pos_div.argsort()[-reliab_pos_num:] # reliab_pos_num\n",
    "    sample_num = len(pos_div)\n",
    "    reliab_pos_mask = [False for i in range(sample_num)] # 初始化 \n",
    "    reliab_pos_mask[reliab_pos_idx] = True # 将可信样本处置为True\n",
    "\n",
    "    return reliab_pos_mask, reliab_pos_idx\n",
    "\n",
    "def reliab_neg_pacing(neg_cand_div_avg, neg_div_threshold, pacing_type, sample_num, t, T):\n",
    "    # 模型的相对容量\n",
    "    relative_cap = max(1, neg_cand_div_avg/neg_div_threshold) \n",
    "\n",
    "    # 根据pacing type进行计算，pacing type \\in [logarithmic, polynomial_1, polynomial_2, polynomial_3]\n",
    "    if pacing_type==\"logarithmic\":\n",
    "        reliab_num = (1 + 0.1 log(relative_cap * (t/T) + e ** (-10))) * sample_num\n",
    "    elif pacing_type==\"polynomial_1\":\n",
    "        reliab_num = (relative_cap * (t/T)) * sample_num\n",
    "    elif pacing_type==\"polynomial_2\":\n",
    "        reliab_num = (relative_cap * (t/T))**2 * sample_num\n",
    "    elif pacing_type==\"polynomial_3\":\n",
    "        reliab_num = (relative_cap * (t/T))**3 * sample_num\n",
    "    else:\n",
    "        error\n",
    "    return reliab_num\n",
    "\n",
    "def get_reliab_neg_mask(neg_div, reliab_neg_num):\n",
    "    sample_num = neg_div.shape[0]\n",
    "    reliab_neg_mask = torch.zeros(sample_num, sample_num).bool()\n",
    "\n",
    "    neg_div = neg_div.reshape(-1)\n",
    "    reliab_neg_idx = neg_div.argsort()[:reliab_neg_num] \n",
    "    reliab_neg_row_idx = []\n",
    "    reliab_neg_col_idx = []\n",
    "    for idx in reliab_neg_idx:\n",
    "        i = idx / sample_num # 取商\n",
    "        j = idx % sample_num # 取模\n",
    "        reliab_neg_mask[i][j] = True\n",
    "        reliab_neg_row_idx.append(i)\n",
    "        reliab_neg_col_idx.append(j)\n",
    "    return reliab_neg_mask, reliab_neg_row_idx, reliab_neg_col_idx\n",
    "\n",
    "def get_cluster_num(prot_2_cluster):\n",
    "    if -1 not in prot_2_cluster:\n",
    "        # print(\"-1 not in\")\n",
    "        cluster_num = max(prot_2_cluster)+1\n",
    "    else:\n",
    "        # print(\"-1 in\")\n",
    "        unnoisy_cluster_num = max(prot_2_cluster)+1\n",
    "        noisy_cluster_num = sum(i==-1 for i in prot_2_cluster)\n",
    "        cluster_num = unnoisy_cluster_num + noisy_cluster_num\n",
    "    return cluster_num\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "break\n",
      "[0, 2, 1, 1, 3]\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# prot_2_cluster = [0,1,1,2,1]\n",
    "# prot_2_cluster_dict=dict()\n",
    "# for i in range(len(prot_2_cluster)):\n",
    "#     prot_2_cluster_dict[i]=prot_2_cluster[i]\n",
    "# for key,value in prot_2_cluster_dict.items():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# prot_2_cluster_dict = dict(sorted(prot_2_cluster_dict.items(),key=lambda x:x[1]))\n",
    "# for key,value in prot_2_cluster_dict.items():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "prot_2_cluster = [0, -1, 1, 1, -1] #总共有1+1+2=4类\n",
    "# if -1 not in prot_2_cluster:\n",
    "#     print(\"-1 not in\")\n",
    "#     cluster_num = max(prot_2_cluster)+1\n",
    "# else:\n",
    "#     print(\"-1 in\")\n",
    "#     unnoisy_cluster_num = max(prot_2_cluster)+1\n",
    "#     noisy_cluster_num = sum(i==-1 for i in prot_2_cluster)\n",
    "#     cluster_num = unnoisy_cluster_num + noisy_cluster_num\n",
    "# print(cluster_num)\n",
    "unnoisy_cluster_num = max(prot_2_cluster)+1\n",
    "noisy_cluster_idx = unnoisy_cluster_num\n",
    "noisy_cluster_num = len([1 for i in prot_2_cluster if i<0])#sum(i==-1 for i in prot_2_cluster)\n",
    "print(noisy_cluster_num)\n",
    "# cluster_num = unnoisy_cluster_num + noisy_cluster_num\n",
    "for i in range(len(prot_2_cluster)):\n",
    "    print(i)\n",
    "    if prot_2_cluster[i] != -1:\n",
    "        continue\n",
    "    prot_2_cluster[i] = noisy_cluster_idx\n",
    "    noisy_cluster_idx += 1\n",
    "    noisy_cluster_num -= 1\n",
    "    if noisy_cluster_num==0: # prot_2_cluster中的-1已经被遍历完成\n",
    "        print(\"break\")\n",
    "        break\n",
    "print(prot_2_cluster)\n",
    "print(unnoisy_cluster_num)\n",
    "print(noisy_cluster_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "prot_2_cluster = [0, -1, 1, -1, -1, -1]\n",
    "unnoisy_cluster_num = max(prot_2_cluster)+1\n",
    "noisy_cluster_idx = unnoisy_cluster_num\n",
    "noisy_cluster_num = sum(i==-1 for i in prot_2_cluster)\n",
    "prot_num = len(prot_2_cluster)\n",
    "# cluster_num = unnoisy_cluster_num + noisy_cluster_num\n",
    "for i in range(prot_num):\n",
    "    if prot_2_cluster[i] == -1:\n",
    "        prot_2_cluster[i] = noisy_cluster_idx\n",
    "        noisy_cluster_idx += 1\n",
    "        noisy_cluster_num -= 1\n",
    "        if noisy_cluster_num==0: # prot_2_cluster中的-1已经被遍历完成\n",
    "            break\n",
    "print(prot_2_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 in\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99921831b712289c3057999129627ce4815d0da040a7927b5b82bc55f53aefa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
